<section id="examples" class="examples">
    <div class="container">
        <div class="section-header">
            <h2>Examples</h2>
            <p>Learn by example with our comprehensive code samples</p>
        </div>
        <div class="examples-tabs">
            <div class="tab-buttons">
                <button class="tab-btn active" onclick="showTab('basic')">Basic Usage</button>
                <button class="tab-btn" onclick="showTab('custom')">Custom Components</button>
                <button class="tab-btn" onclick="showTab('advanced')">Advanced Configuration</button>
            </div>
            
            <div id="basic" class="tab-content active">
                <h3>Basic Image Classification</h3>
                <div class="example-description">
                    <p>A simple example showing how to create and train a basic neural network for image classification using CIFAR-10 dataset.</p>
                </div>
                <div class="code-block">
                    <pre><code class="language-python">import torchmodular as tm
import torchvision
import torchvision.transforms as transforms

# Load CIFAR-10 dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

trainset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True, transform=transform
)
trainloader = tm.DataLoader(trainset, batch_size=32, shuffle=True)

# Define the model
model = tm.Sequential([
    tm.Flatten(),
    tm.Linear(32*32*3, 512),
    tm.ReLU(),
    tm.Dropout(0.2),
    tm.Linear(512, 256),
    tm.ReLU(),
    tm.Linear(256, 10)
])

# Setup training
optimizer = tm.Adam(model.parameters(), lr=0.001)
criterion = tm.CrossEntropyLoss()
trainer = tm.Trainer(model, optimizer, criterion)

# Train the model
trainer.fit(trainloader, epochs=10)

# Evaluate
accuracy = trainer.evaluate(testloader)
print(f"Test Accuracy: {accuracy:.4f}")</code></pre>
                </div>
            </div>
            
            <div id="custom" class="tab-content">
                <h3>Custom Layer Implementation</h3>
                <div class="example-description">
                    <p>Learn how to create custom layers and integrate them seamlessly with the TorchModular ecosystem.</p>
                </div>
                <div class="code-block">
                    <pre><code class="language-python">import torchmodular as tm
import torch.nn as nn
import torch.nn.functional as F

class CustomAttentionLayer(tm.Module):
    """Custom self-attention layer implementation"""
    
    def __init__(self, embed_dim, num_heads=8):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        self.qkv = tm.Linear(embed_dim, embed_dim * 3, bias=False)
        self.proj = tm.Linear(embed_dim, embed_dim)
        self.dropout = tm.Dropout(0.1)
    
    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)
        q, k, v = qkv.unbind(0)
        
        # Scaled dot-product attention
        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)
        attn = attn.softmax(dim=-1)
        attn = self.dropout(attn)
        
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        return x

# Use the custom layer in a model
model = tm.Sequential([
    tm.Embedding(vocab_size, embed_dim),
    CustomAttentionLayer(embed_dim, num_heads=8),
    tm.LayerNorm(embed_dim),
    tm.Linear(embed_dim, num_classes)
])

# Train as usual
trainer = tm.Trainer(model, optimizer, criterion)
trainer.fit(dataloader)</code></pre>
                </div>
            </div>
            
            <div id="advanced" class="tab-content">
                <h3>Advanced Training Configuration</h3>
                <div class="example-description">
                    <p>Advanced example showing how to use callbacks, custom metrics, and distributed training features.</p>
                </div>
                <div class="code-block">
                    <pre><code class="language-python">import torchmodular as tm
from torchmodular.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler

# Define a more complex model
class ResNetBlock(tm.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = tm.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)
        self.bn1 = tm.BatchNorm2d(out_channels)
        self.conv2 = tm.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
        self.bn2 = tm.BatchNorm2d(out_channels)
        
        self.shortcut = tm.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = tm.Sequential([
                tm.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                tm.BatchNorm2d(out_channels)
            ])
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

# Build model with custom blocks
model = tm.Sequential([
    tm.Conv2d(3, 64, 7, 2, 3, bias=False),
    tm.BatchNorm2d(64),
    tm.ReLU(),
    tm.MaxPool2d(3, 2, 1),
    ResNetBlock(64, 64),
    ResNetBlock(64, 128, 2),
    ResNetBlock(128, 256, 2),
    tm.AdaptiveAvgPool2d((1, 1)),
    tm.Flatten(),
    tm.Linear(256, 1000)
])

# Advanced training configuration
callbacks = [
    EarlyStopping(patience=10, monitor='val_loss'),
    ModelCheckpoint(filepath='best_model.pth', monitor='val_accuracy'),
    LearningRateScheduler(schedule='cosine', max_epochs=100)
]

# Custom metrics
metrics = [
    tm.Accuracy(),
    tm.F1Score(num_classes=1000),
    tm.TopKAccuracy(k=5)
]

# Advanced trainer configuration
trainer = tm.Trainer(
    model=model,
    optimizer=tm.AdamW(model.parameters(), lr=0.001, weight_decay=0.01),
    criterion=tm.CrossEntropyLoss(label_smoothing=0.1),
    callbacks=callbacks,
    metrics=metrics,
    mixed_precision=True,
    gradient_clipping=1.0,
    accumulate_grad_batches=4
)

# Train with validation
trainer.fit(
    train_dataloader,
    val_dataloader,
    epochs=100,
    log_every_n_steps=10
)</code></pre>
                </div>
            </div>
        </div>
    </div>
</section>
